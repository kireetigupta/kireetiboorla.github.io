---
layout: post
title: Apache Spark
category: tutorials
---

# Spark vs Map Reduce
Map Reduce is a batch process oriented framework not suitable for iterative jobs.
Map Reduce also isn't suitable for streaming processing.
Apache Spark tries to achieve these through in memory computation and also helps
run distributed applications across heterogeneous cluster managers(Standalone,YARN, Meoses)

# Components in Spark
1. Core - responsible for IO and other stuff. RDD is the basic block of operation. Allows for in-memory computation. Operations on RDDs are transformations, actions.
2. SQL- Queries on distributed data in form of DataFrame(collection of data into set of columns). Catalyst optimizer to optimize queries and execution.
3. Spark Streaming - provides streaming API to help process data inflow in real time in terms of batches(Discretized streams).
4. MLlib - set of ML algorithms implemented on top using Spark core providing the fast processing and other features of Spark. API in python,scala and Java. Already has implementations for popular ML algorithms. Complications that arise in distributed implementations of ML algorithms are taken care of.
5. GraphX - API for graph transformation, construction and computations.


# Installing Spark and Standalone Mode
1. Need Scala :
  1. Verify : scala -version
  2. Install: apt-get install scala (on success : scala code runner installed)
2. Scala:
  1. download Spark
  2. Extract and place in /usr/local/spark
  3. Add /usr/local/spark/bin to PATH
  4. scala-shell to access Spark shell.


## Modes of deployment
1. Standalone mode where Spark directly sits on HDFS and does the stuff, even scheduling etc.
2. On YARN, where the application is submitted to YARN ?

# Hello World in Spark
```scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark._

object SparkWordCount {
   def main(args: Array[String]) {

    val sc = new SparkContext( "local", "Word Count", "/usr/local/spark", Nil, Map(), Map())           
      val input = sc.textFile("input.txt")             
      Val count = input.flatMap(line ⇒ line.split(" "))
      .map(word ⇒ (word, 1))
      .reduceByKey(_ + _)       
      count.saveAsTextFile("outfile")
      System.out.println("OK");
   }
}
```

# [Getting Started with Spark. ](https://spark.apache.org/docs/2.3.1/quick-start.html)

Main driver function runs the application over the cluster.
Spark provides two important abstraction namely Datasets and variables.
1. Datasets is the main abstraction that represents the data built from files on HDFS or in-application collection. Spark provides transformations and operations to create, modify datasets.
2. Variables are application variables that are made available to nodes across the application's runtime. There are two types of variables 1. broadcast variables 2. accumulators.

### Linking with Spark.
To link Spark to your application you need:
1. core Spark dependency
```maven
groupId = org.apache.spark
artifactId = spark-core_2.11
version = 2.3.1
```
2. hadoop-client to connect with HDFS
3. SparkContext, SparkConf classes

Spark needs few important information before the application starts running.
The master- which gives the cluster information ("local" for Standalone mode).
You can set jars in JVM's class path using --packages option and similarly maven dependencies.

# RDD
## Creating RDD
One way is to use **Parallelized Collections** which are created by parallelizing the collections in the driver program.

```
val data=Array(1,2,3,4,5)
val distData=sc.parallelize(data)
```
This will make the _distData_ be available across the nodes of the applications.

## External DataSets
Another way is to use any filesystem(local/distributed) file or anyother Hadoop InputFormat files as sources.
```
val distFile=sc.textFile("data.txt")
```
Note that above textFile is used to only load data from textfiles. It takes another second parameter, the number of partitions.

## RDD operations




## Exercise in Spark.
I will try to  analyze Bitcoin Alpha web of trust network using Spark and Scala on the Databricks platform.  This graph is a who-trusts-whom network of people who trade using Bitcoin on a platform called Bitcoin Alpha. For a node, the number of incoming edges is the in-degree of the node and the number of outgoing edges is called the out-degree.  The total degree is the sum of all edges for a node.  

Using Databricks I will try to find the most trustworthy user, the user who trusts most other users and the one with highest interactions.





### Resources to learn
1. [Idea of why and what Spark is.](https://www.dezyre.com/apache-spark-tutorial/tutorial-introduction-to-apache-spark)
